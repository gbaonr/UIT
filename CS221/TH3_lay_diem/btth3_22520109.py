# -*- coding: utf-8 -*-
"""22520109_BTTH3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zbDJxZhaLaBAC6vpwlNasV7_qViiiaWK
"""
# nltk libs
import nltk
from nltk.probability import LidstoneProbDist, WittenBellProbDist, LaplaceProbDist
from nltk.tag.hmm import HiddenMarkovModelTrainer
from collections import Counter


def filter_rare_words(traintext, trainlabel, testtext, threshold=1):
    # Đếm từ xuất hiện trong traintext
    word_counts = Counter(word for sent in traintext for word in sent)
    # Lấy set các từ đủ tần suất
    common_words = {word for word, count in word_counts.items() if count >= threshold}

    def replace_sent(sent):
        return [word if word in common_words else "<UNK>" for word in sent]

    # Lọc traintext + testtext
    traintext_new = [replace_sent(sent) for sent in traintext]
    testtext_new = [replace_sent(sent) for sent in testtext]

    return traintext_new, trainlabel, testtext_new


def run(traintext, trainlabel, testtext):
    # Lọc từ hiếm trước khi train
    traintext, trainlabel, testtext = filter_rare_words(
        traintext, trainlabel, testtext, threshold=2
    )

    train_sents = [
        [(word, tag) for word, tag in zip(traintext[i], trainlabel[i])]
        for i in range(len(traintext))
    ]
    tagger = None
    try:
        tagger = HiddenMarkovModelTrainer().train_supervised(
            train_sents, estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins)
        )
        print("Using LidstoneProbDist")
    except Exception as e:
        print(f"Failed Lidstone, try WittenBell: {e}")
        try:
            tagger = HiddenMarkovModelTrainer().train_supervised(
                train_sents, estimator=lambda fd, bins: WittenBellProbDist(fd, bins)
            )
            print("Using WittenBellProbDist")
        except Exception as e2:
            print(f"Failed WittenBell, try Laplace: {e2}")
            try:
                tagger = HiddenMarkovModelTrainer().train_supervised(
                    train_sents, estimator=lambda fd, bins: LaplaceProbDist(fd, bins)
                )
                print("Using LaplaceProbDist")
            except Exception as e3:
                print(f"Failed all HMMs, fallback Unigram: {e3}")
                unigram_tagger = nltk.UnigramTagger(train_sents)
                return [
                    [tag if tag else "NN" for _, tag in unigram_tagger.tag(sent)]
                    for sent in testtext
                ]

    # Apply HMM tagger
    results = []
    for sent in testtext:
        tagged_sent = tagger.tag(sent)
        tags = [t[1] for t in tagged_sent]
        results.append(tags)
    return results
