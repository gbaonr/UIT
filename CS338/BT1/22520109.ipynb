{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA DESCRIPTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# load data and describe \n",
    "file = pd.read_csv('dirty_cafe_sales.csv')\n",
    "df = pd.DataFrame(file)\n",
    "df.info()\n",
    "df.describe()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# consistificate missing values to NaN\n",
    "df.replace([\"ERROR\", \"UNKNOWN\", \"\", \" \"], np.nan, inplace=True)\n",
    "\n",
    "# Identify missing values\n",
    "new_missing_values = df.isnull().sum()\n",
    "print(\"Missing values count: \\n\",new_missing_values)\n",
    "new_missing_percent = df.isnull().sum() / len(df) * 100 \n",
    "print(\"\\nNew missing percent: \\n\", new_missing_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# visualize missing data\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(df.isna(), cmap='viridis', cbar=False)\n",
    "plt.title('Missing data in the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap cho thấy cột Payment Method và Location là có đa số phần tử đều bị thiếu. Riêng cột Transaction ID thì không có giá trị bị thiếu. Nhưng cột này và Transaction Date lại không đóng góp trong quá trình huấn luyện, thậm chí gây nhiễu, nên sẽ bị bỏ sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "missing_counts = df.isnull().sum()/len(df)*100\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Missing percent per Column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biểu đồ cho thấy số lượng mẫu bị thiếu giá trị Payment Method và Location rất cao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# count types of values for Item, Payment Method, Location\n",
    "item_counts = df['Item'].value_counts().reset_index()\n",
    "item_counts.columns = ['Item', 'Count']\n",
    "\n",
    "payment_counts = df['Payment Method'].value_counts().reset_index()\n",
    "payment_counts.columns = ['Payment Method', 'Count']\n",
    "\n",
    "location_counts = df['Location'].value_counts().reset_index()\n",
    "location_counts.columns = ['Location', 'Count']\n",
    "\n",
    "# visualize the counts\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Pie chart for Item\n",
    "ax1.pie(item_counts['Count'], labels=item_counts['Item'], autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Types of Item values')\n",
    "\n",
    "# Pie chart for Payment Method\n",
    "ax2.pie(payment_counts['Count'], labels=payment_counts['Payment Method'], autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Types of Payment Method values')\n",
    "\n",
    "# Pie chart for Location\n",
    "ax3.pie(location_counts['Count'], labels=location_counts['Location'], autopct='%1.1f%%', startangle=90)\n",
    "ax3.set_title('Types of Location values')\n",
    "\n",
    "# show\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Biểu dồ trên thể hiện các giá trị có thể có của 3 cột Items, Payment Method và Location. Đây là 3 cột duy có dữ liệu kiểu string, cần áp dụng Label Encoding để tiện cho quá trình huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# count types of values for Item, Payment Method, Location\n",
    "item_counts = df['Quantity'].value_counts().reset_index()\n",
    "item_counts.columns = ['Q', 'Count']\n",
    "\n",
    "payment_counts = df['Price Per Unit'].value_counts().reset_index()\n",
    "payment_counts.columns = ['P', 'Count']\n",
    "\n",
    "location_counts = df['Total Spent'].value_counts().reset_index()\n",
    "location_counts.columns = ['T', 'Count']\n",
    "\n",
    "# visualize the counts\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Pie chart for Item\n",
    "ax1.pie(item_counts['Count'], labels=item_counts['Q'], autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Types of Quantity values')\n",
    "\n",
    "# Pie chart for Payment Method\n",
    "ax2.pie(payment_counts['Count'], labels=payment_counts['P'], autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Types of Price Per Unit values')\n",
    "\n",
    "# Pie chart for Location\n",
    "ax3.pie(location_counts['Count'], labels=location_counts['T'], autopct='%1.1f%%', startangle=90)\n",
    "ax3.set_title('Types of Total Spent values')\n",
    "\n",
    "# show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 biểu đồ trên cho thấy, cột Quantiy và Price Per Unit sẽ chỉ bao gồm các giá trị cụ thể, không thực sự biến thiên nhiều. Điều này sẽ giúp cho mô hình dự đoán các giá trị này tốt hơn. Ở chiều ngược lại, cột Total Spent có sự biến thiên giá trị lớn hơn, nên kết quả sẽ không được tốt bằng 2 cột còn lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# visualize the distribution of the numerical columns\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(25, 12))  # 1 row, 3 columns\n",
    "\n",
    "df['Quantity'] = df['Quantity'].astype(float)\n",
    "df['Price Per Unit'] = df['Price Per Unit'].astype(float)\n",
    "df['Total Spent'] = df['Total Spent'].astype(float)\n",
    "# Distribution of Quantity\n",
    "sns.histplot(df['Quantity'], ax=ax1, color='skyblue')\n",
    "ax1.set_title('Distribution of Quantity')\n",
    "\n",
    "sns.histplot(df['Price Per Unit'], ax=ax2, color='coral')\n",
    "ax2.set_title('Distribution of Price Per Unit')\n",
    "\n",
    "sns.histplot(df['Total Spent'], ax=ax3, color='green')\n",
    "ax3.set_title('Distribution of Total Spent')\n",
    "\n",
    "sns.histplot(df['Item'], ax=ax4, color='gray')\n",
    "ax4.set_title('Distribution of Item')\n",
    "\n",
    "sns.histplot(df['Payment Method'], ax=ax5, color='pink')\n",
    "ax5.set_title('Distribution of Payment Method')\n",
    "\n",
    "sns.histplot(df['Location'], ax=ax6, color='purple')\n",
    "ax6.set_title('Distribution of Location')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Price Consistency:**\n",
    "\n",
    "Prices for menu items are consistent but may have missing or incorrect values introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# menu prices from datacard\n",
    "dicts ={\n",
    "    \"Coffee\": 2,\n",
    "    \"Tea\": 1.5,\n",
    "    \"Sandwich\": 4,\n",
    "    \"Salad\": 5,\n",
    "    \"Cake\": 3,\n",
    "    \"Cookie\": 1,\n",
    "    \"Smoothie\": 4,\n",
    "    \"Juice\": 3,\n",
    "}\n",
    "\n",
    "# search for incorrect values\n",
    "incorrect_price = []\n",
    "incorrect_total = []\n",
    "for index, row in df.iterrows():\n",
    "    item = str(row['Item'])\n",
    "    if item not in dicts:\n",
    "        continue\n",
    "    \n",
    "    if pd.isna(row['Price Per Unit']) or pd.isna(row['Total Spent']) or pd.isna(row['Quantity']):\n",
    "        continue\n",
    "\n",
    "    # price per unit check\n",
    "    true_price = float(dicts.get(item))\n",
    "    if float(row['Price Per Unit']) != true_price:\n",
    "        incorrect_price.append(index)\n",
    "\n",
    "    # total spent check\n",
    "    quantity = float(row['Quantity'])\n",
    "    total = float(row['Total Spent'])\n",
    "    if total != quantity * true_price:\n",
    "        incorrect_total.append(index)\n",
    "print(\"Number of incorrect price per unit: \",len(incorrect_price))\n",
    "print(\"Number of incorrect total spent: \", len(incorrect_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--> Dữ liệu không bị sai lệch bảng giá, chỉ bị thiếu dữ liệu ở các dòng**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Problem Choosing: Predict quantity of item for each transaction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MISSING VALUES HANDLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Drop transaction ID, Transaction Date, Total Spent -> these columns are not useful for model training\n",
    "df.drop(['Transaction ID', 'Transaction Date', 'Total Spent'], axis=1, inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# split data into 2 sets: clear data and dirty data\n",
    "# clear data: used to create test set (2000 samples)\n",
    "# dirty data: used to create training set (8000 samples)\n",
    "clear_data = df.dropna()    \n",
    "\n",
    "\n",
    "print(\"CLEAR DATA INFO:\")\n",
    "print(clear_data.info())\n",
    "\n",
    "test_data = clear_data.sample(n=2000, random_state=22520109)\n",
    "train_data = df.drop(test_data.index)\n",
    "\n",
    "print(\"\\n\\nTEST DATA INFO:\")\n",
    "print(test_data.info())\n",
    "\n",
    "\n",
    "print(\"\\n\\nTRAIN DATA INFO:\")\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Method 1: Drop rows with missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "train_data1 = train_data.copy()\n",
    "train_data1.dropna(inplace=True)\n",
    "print(\"\\n\\nTRAIN DATA1 missing values count:\")\n",
    "print(train_data1.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# compare the number of rows with and without missing values\n",
    "rows_with_missing = train_data.isnull().any(axis=1).sum()\n",
    "rows_complete = len(train_data) - rows_with_missing\n",
    "\n",
    "\n",
    "labels = ['Rows with at least\\n 1 Missing Values', 'Rows with No \\nMissing Values']\n",
    "values = [rows_with_missing, rows_complete]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(labels, values, color=['salmon', 'lightgreen'])\n",
    "plt.title('Number of Rows with vs without \\nMissing Values in Training Data')\n",
    "plt.ylabel('Number of Rows')\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 100, str(v), ha='center')  # write the number of each bar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuy số lượng hàng bị thiếu dữ liệu ở từng cột không nhiều, nhưng khi so sánh trên toàn bộ các cột, số lượng hàng có ít nhất 1 trường dữ liệu bị trống là rất lớn (6763 hàng), nên ta nếu ta bỏ đi toàn bộ những hàng này, tập train sẽ còn lại rất ít mẫu, kết quả từ mô hình huấn luyện trên tập dữ liệu này sẽ không được tốt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do dữ liệu gồm các cột chứa giá trị số và các cột chứa giá trị chuỗi, nên ta sẽ dùng mô hình Linear Regression để huấn luyện dự đoán các cột có giá trị số và Logistic Regression cho các cột có giá trị chuỗi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# choose a column as label, others as features\n",
    "def create_input_LogReg(data, selected_column):\n",
    "    # print(\"\\nCreating input for Logistic Regression model...\")\n",
    "    # choose label\n",
    "    numeric_features = ['Quantity', 'Price Per Unit']\n",
    "    string_features = ['Item', 'Payment Method', 'Location']\n",
    "    if selected_column in string_features:\n",
    "        string_features.remove(selected_column)\n",
    "    else:\n",
    "        numeric_features.remove(selected_column)\n",
    "\n",
    "    # split data into features and label\n",
    "    y = data[selected_column]\n",
    "    X = data.drop(selected_column, axis=1)\n",
    "\n",
    "    # one-hot encoding for string features and scaling for numerical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False), string_features)\n",
    "        ])\n",
    "    X = preprocessor.fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # print(\"Shape of data after encoding: \", X.shape)\n",
    "    # print(X.columns)\n",
    "\n",
    "\n",
    "    # label encoding for label\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    y = pd.DataFrame(y)\n",
    "    # print(\"Shape of label after encoding: \", y.shape, \"\\n\")\n",
    "\n",
    "    # return\n",
    "    return X, y, le\n",
    "    \n",
    "\n",
    "def create_input_LinReg(data, selected_column):\n",
    "    # print(f\"\\nCreating input for {selected_column}...\")\n",
    "\n",
    "    # choose label\n",
    "    numeric_features = ['Quantity', 'Price Per Unit']\n",
    "    string_features = ['Item', 'Payment Method', 'Location']\n",
    "    if selected_column in string_features:\n",
    "        string_features.remove(selected_column)\n",
    "    else:\n",
    "        numeric_features.remove(selected_column)\n",
    "\n",
    "    # split data into features and label\n",
    "    y = data[selected_column]\n",
    "    X = data.drop(selected_column, axis=1)\n",
    "    # X.drop(['Payment Method', 'Location', 'Item'], axis=1, inplace=True)\n",
    "    # print(X.columns)\n",
    "\n",
    "    # one-hot encoding for string features and scaling for numerical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(sparse_output=False), string_features)\n",
    "        ])\n",
    "    X = preprocessor.fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # return mean of y \n",
    "    mean_y = y.astype(float).mean()\n",
    "\n",
    "\n",
    "    # print(\"Shape of data after encoding: \", X.shape)\n",
    "\n",
    "    # return\n",
    "    return X, y, mean_y\n",
    "\n",
    "\n",
    "# X_train, y_train = create_input_LogReg(train_data1, select)\n",
    "# X_test, y_test = create_input_LogReg(test_data, select)\n",
    "\n",
    "\n",
    "select = \"Quantity\"\n",
    "X_train, y_train, _ = create_input_LinReg(train_data1, select)\n",
    "X_test, y_test, _ = create_input_LinReg(test_data, select)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "\n",
    "print(y_train.head())\n",
    "\n",
    "# embeded columns order: Price Per Unit, Item, Payment Method, Location\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train and evaluate models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error\n",
    "\n",
    "# Logistic Regression model\n",
    "def logistic_regression(selected_column, train_data, max_iter=1000):\n",
    "    X_train, y_train, le_train = create_input_LogReg(train_data, selected_column)\n",
    "    X_test, y_test, le_test = create_input_LogReg(test_data, selected_column)\n",
    "    model = LogisticRegression(max_iter=max_iter)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy for {selected_column} \", accuracy)\n",
    "\n",
    "def linear_regression(selected_column, train_data):\n",
    "    X_train, y_train, y_train_mean = create_input_LinReg(train_data, selected_column)\n",
    "    X_test, y_test, y_test_mean = create_input_LinReg(test_data, selected_column)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = root_mean_squared_error(y_test, y_pred)\n",
    "    print(f\"RMSE for {selected_column} \", mse)\n",
    "    print(f\"\\tMean of {selected_column} in train data: \", y_train_mean)\n",
    "    print(f\"\\tMean of {selected_column} in test data: \", y_test_mean)\n",
    "    print(f\"\\tRMSE/y_test_mean: {(mse/y_test_mean):.2f}, RMSE/y_train_mean: {(mse/y_train_mean):.2f}\")\n",
    "\n",
    "# string_columns = ['Item', 'Payment Method', 'Location']\n",
    "# numeric_columns = ['Price Per Unit', 'Quantity', 'Total Spent']\n",
    "# for column in string_columns:\n",
    "#     logistic_regression(column, train_data1, 2000)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# for column in numeric_columns:\n",
    "#     linear_regression(column, train_data1\n",
    "\n",
    "linear_regression('Quantity', train_data1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kết luận:**\n",
    "\n",
    "<!-- - Cột Item: giá trị này tạm chấp nhận được. Do mỗi nhãn trong Item đều tương ứng với 1 giá trị giá (có trùng nhau), có thể tính toán được khi có giá trị Quantity và Total Spent, nên mô hình có thể học được mối quan hệ này, mặc dù dữ liệu huấn luyện quá ít so với dữ liệu test -->\n",
    "<!-- - Cột Payment Method và Location: 2 cột này không hề có quan hệ rõ ràng với các trường dữ liệu nên kết quả thu được thấp hơn chủ yếu là do sự chênh lệch số lượng giữa tập train và tập test -->\n",
    "<!-- - Cột Price Per Unit: Tỉ lệ RMSE/mean đạt được gần như là 0, cho thấy mô hình gần như hoàn hảo. Điều này có thể lý giải do mối quan hệ *Total Spent = Price Per Unit * Quantity* được thể hiện chặt chẽ trong các samples. Thêm vào đó, các giá trị Price Per Unit trong dữ liệu có rất ít biến thiên cũng góp phần vào sự hoàn hảo của mô hình -->\n",
    "- RMSE cho cột Quantity: Tỉ lệ RMSE/mean = 0.48 thể hiện sai số trung bình của mô hình chiếm 48% so với dữ liệu thực tế, đây là một kết quả rất tệ. Có thể thấy, sau khi bỏ đi cột Total Spent, mối tương quan *Total Spent = Quantity * Price Per Unit* đã không còn, cộng thêm việc dữ liệu của cột Quantity không có phân phối chuẩn, nên model linear regression không hoạt động tốt. Việc bỏ đi các dòng bị thiếu dữ liệu khiến kích thước tập train nhỏ hơn test, điều này cũng làm cho độ lỗi tăng lên do mô hình chưa có nhiều dữ liệu để học."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Method 2: Mean/Median Imputation**\n",
    "\n",
    "Do có một số cột dữ liệu là kiểu chuỗi nên ta không thể dùng Mean hay Median, thay vào đó, ta dùng Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# make a new copy of train data and fill missing values\n",
    "train_data2 = train_data.copy()\n",
    "# print(train_data2.info())\n",
    "\n",
    "\n",
    "string_columns = ['Item', 'Payment Method', 'Location']\n",
    "numeric_columns = ['Price Per Unit', 'Quantity']\n",
    "print(\"Filling string columns with mode and numeric columns with mean...\")\n",
    "for column in string_columns:\n",
    "    print(f\"\\t{column}.mode() = {train_data2[column].mode()[0]}\")\n",
    "    train_data2[column] = train_data2[column].fillna(train_data2[column].mode()[0])\n",
    "\n",
    "for column in numeric_columns:\n",
    "    train_data2[column] = pd.to_numeric(train_data2[column], errors='coerce')\n",
    "    # replace_value = train_data2[column].median()\n",
    "    replace_value = train_data2[column].mean()\n",
    "    print(f\"\\t{column}.mean() = {replace_value}\")\n",
    "    train_data2[column] = train_data2[column].fillna(replace_value)\n",
    "\n",
    "missing_values_count = train_data2.isna().sum()\n",
    "print(\"\\n\\nTRAIN DATA2 missing values count:\")  \n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train and evaluate models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# for column in string_columns:\n",
    "#     logistic_regression(column, train_data2, 2000)\n",
    "\n",
    "# for column in numeric_columns:\n",
    "#     linear_regression(column, train_data2)\n",
    "\n",
    "linear_regression('Quantity', train_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kết luận:**\n",
    "\n",
    "<!-- Nhìn chung, kết quả của phương pháp này dành cho các cột có giá trị chuỗi thấp hơn đáng kể so với phương pháp Drop rows with missing data, trong khi các cột có giá trị số thì vẫn ở mức khá tốt mặc dù có thấp hơn một chút. \n",
    "\n",
    "Lý giải cho điều này, đối với các cột chứa dữ liệu chuỗi, đặc biệt là cột Item (bị giảm đáng kể accuracy), khi điền các vị trí bị thiếu bằng Mode của cột, các giá trị này hầu như sẽ không đúng, những giá trị sai lệch này sẽ ảnh hưởng đến bảng giá trong menu khiến cho giá của sản phẩm bị lẫn lộn, model không thể học được sự liên quan giữa Item và Price Per Unit, khiến cho Acccuracy bị giảm mạnh. Còn lại các cột Payment Method và Location, do không có mối quan hệ rõ ràng nào với các trường dữ liệu khác, cộng thêm phần lớn các dòng bị thiếu dữ liệu, nên việc điền bằng Mode cũng làm nhiễu đi thông tin, khiến kết quả còn thấp hơn cả phương pháp trước đó mặc dù có nhiều dữ liệu huấn luyện hơn.\n",
    "\n",
    "Đối với các cột chứa dữ liệu số, kết quả vẫn ở mức tốt do có quan hệ giữa các cột Quantity, Payment Method và Price Per Unit. Nhưng kết quả có phần giảm nhẹ so với phương pháp trước đó, điều này là do tập train nhiều dữ liệu hơn, nhưng lại là các giá trị được nội suy, không đảm bảo tính đúng đắn, làm nhiều quá trình học được mối quan hệ giữa các cột. -->\n",
    "\n",
    "Ở phương pháp này số lượng mẫu của tập train đã có nhiều hơn, nhưng thực tế thì phần lớn chúng là các giá trị được điền vào bằng giá trị mode/mean. Những giá trị này đa phần sẽ không đúng và gây nhiễu cho việc học của mô hình. Vậy nên kết quả thu được vẫn chưa được tốt, mặc dù có cải thiện 1%.\n",
    "\n",
    "Điều tương tự cũng xảy ra khi thay Mean bằng Median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Method 3: Advanced Techniques**\n",
    "\n",
    "Dùng các mô hình máy học để dự đoán các giá trị khuyết.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Sự khác biệt cơ bản giữa Custom KNN và KNNImputer:***\n",
    "\n",
    "- Custom KNN: Chia tập train thành 2 phần: complete (không có NaN) và missing (chỉ chứa các hàng có ít nhất 1 NaN). Dùng tập complete để train KNN, sau đó dùng model KNN đã được huấn luyện để dự đoán các giá trị bị thiếu trong tập missing. Vấn đề gặp phải là các hàng trong tập missing có khả năng bị *thiếu dữ liệu ở các features columns* (xảy ra khi hàng đó thiếu trên 2 trường dữ liệu). Cách xử lý: Điền tạm các giá trị bị thiếu trong các features column bằng Mean/Mode.\n",
    "\n",
    "- KNNImputer: dùng thằng tập train ban đầu, không cần điền tạm. Khi dùng lệnh fit, KNNImputer sẽ lặp qua từng dòng trong tập dữ liệu được, với mỗi giá trị bị thiếu ở 1 cột, nó sẽ tìm *k* hàng ở gần nhất trong không gian để tham chiếu, sau đó lấy trung bình giá trị ở *k* hàng đó và điền vào trường bị thiếu trong hàng mục tiêu. Không gặp phải vấn đề thiếu dữ liệu ở các features columns vì KNNImputer chỉ thực hiện tính toán đối với các hàng khác có trùng các cột không bị thiếu dữ liệu với hàng mục tiêu. Ví dụ hàng mục tiêu thiếu các giá trị x1, x2 và có đủ các giá trị từ x3 đến x8, và ta đang cần điền vào x1, thì KNNImputer sẽ tìm kiếm các hàng có giá trị x1, thiếu giá trị x2 và có đủ các giá trị từ x3 đến x8 để tính toán khoảng cách trong không gian Euclidean với hàng mục tiêu, sau đó chọn ra *k* hàng gần nhất, lấy trung bình giá trị x1 và điền vào x1 đang bị thiếu của hàng mục tiêu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Custom KNN**\n",
    "\n",
    "- Ý tưởng: Do dữ liệu có giá trị bị khuyết nằm ở nhiều cột, và các dòng có thể có 1 hoặc nhiều cột bị thiếu, nên ta sẽ tách từ train_data ra 1 tập các hàng không bị thiếu giá trị dùng để huấn luyện mô hình KNN, sau đó dùng chính mô hình KNN đã huấn luyện này để dự đoán các giá trị bị thiếu (từng cột một) trong phần còn lại của train_data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# demonstrate how many rows have missing values\n",
    "missing_values_count = train_data.isna().sum(axis=1)\n",
    "missing_type = [0,0,0,0,0,0,0,0]\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    row_missing_count = row.isna().sum()\n",
    "    missing_type[row_missing_count] += 1\n",
    "\n",
    "\n",
    "\n",
    "titles = [\"no missing\", \"1 missing\", \"2 missing\", \"3 missing\", \"4 missing\", \"5 missing\", \"6 missing\", \"7 missing\"]\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.bar(titles, missing_type)\n",
    "for i, v in enumerate(missing_type):\n",
    "    plt.text(i, v + 50, str(v), ha='center')  # write the number of each bar\n",
    "plt.title(\"Number of rows with missing values\")\n",
    "plt.xlabel(\"Number of missing values\")    \n",
    "plt.ylabel(\"Number of rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như ta có thể thấy trên biểu đồ, có một số lượng không nhỏ những hàng bị thiếu trên 2 trường dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "# Convert numeric columns to float\n",
    "train_data['Price Per Unit'] = pd.to_numeric(train_data['Price Per Unit'], errors='coerce')\n",
    "train_data['Quantity'] = pd.to_numeric(train_data['Quantity'], errors='coerce')\n",
    "# train_data['Total Spent'] = pd.to_numeric(train_data['Total Spent'], errors='coerce')\n",
    "# Tách train_data thành 2 phần\n",
    "train_data_complete = train_data.dropna()\n",
    "train_data_missing = train_data[train_data.isna().any(axis=1)]\n",
    "\n",
    "print(\"Train data complete: \", train_data_complete.shape)\n",
    "print(\"Train data missing: \", train_data_missing.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# impute for numeric columns\n",
    "def knn_reg(train_data_complete, train_data_missing, selected_column):\n",
    "    numeric_columns = ['Price Per Unit', 'Quantity']\n",
    "    string_columns = ['Item', 'Payment Method', 'Location']\n",
    "    # create a copy of train_data_missing, doesn't change the original data\n",
    "    df_missing = train_data_missing.copy()\n",
    "    print(f\"Current selected column: {selected_column}\")\n",
    "\n",
    "    # fill features cols in train_data_missing with temp value using mean\n",
    "    for column in numeric_columns:\n",
    "        if column == selected_column:\n",
    "            continue\n",
    "        df_missing[column] = pd.to_numeric(df_missing[column], errors='coerce')\n",
    "        df_missing[column] = df_missing[column].fillna(df_missing[column].mean())\n",
    "\n",
    "    # fill features cols in train_data_missing with temp value using mode\n",
    "    for column in string_columns:\n",
    "        if column == selected_column:\n",
    "            continue\n",
    "        # df_missing[column].fillna({column:train_data_complete[column].mode()[0]}, inplace=True)\n",
    "        df_missing[column] = df_missing[column].fillna(df_missing[column].mode()[0])\n",
    "\n",
    "    # create model input for KNN\n",
    "    X_train, y_train, y_train_mean = create_input_LinReg(train_data_complete, selected_column)\n",
    "    knn = KNeighborsRegressor(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Predict missing values in target_col of train_data_missing\n",
    "    mask = df_missing[selected_column].isna()\n",
    "    if mask.any():\n",
    "        X_missing, _, _ = create_input_LinReg(df_missing[mask], selected_column) # create features for missing values\n",
    "        predicted = knn.predict(X_missing)\n",
    "        df_missing.loc[mask, selected_column] = predicted\n",
    "\n",
    "    return df_missing\n",
    "\n",
    "\n",
    "# impute for string columns\n",
    "def knn_clf(train_data_complete, train_data_missing, selected_column):\n",
    "    string_columns = ['Item', 'Payment Method', 'Location']\n",
    "    numeric_columns = ['Price Per Unit', 'Quantity']\n",
    "    df_missing = train_data_missing.copy()\n",
    "    print(f\"Current selected column: {selected_column}\")\n",
    "    try:\n",
    "        # fill features cols in train_data_missing with temp value using mode\n",
    "        for column in string_columns:\n",
    "            if column == selected_column:\n",
    "                continue\n",
    "            # df_missing[column].fillna({column:train_data_complete[column].mode()[0]}, inplace=True)\n",
    "            df_missing[column] = df_missing[column].fillna(df_missing[column].mode()[0])\n",
    "\n",
    "        # fill features cols in train_data_missing with temp value using mean\n",
    "        for column in numeric_columns:\n",
    "            if column == selected_column:\n",
    "                continue\n",
    "            df_missing[column] = pd.to_numeric(df_missing[column], errors='coerce')\n",
    "            # df_missing[column].fillna({column:train_data_complete[column].mean()}, inplace=True)\n",
    "            df_missing[column] = df_missing[column].fillna(df_missing[column].mean())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"\\tCurrent selected column: \", selected_column)\n",
    "        print(df_missing)\n",
    "    \n",
    "    # create model input for KNN\n",
    "    X_train, y_train, le = create_input_LogReg(train_data_complete, selected_column)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # Predict missing values in target_col of train_data_missing\n",
    "    mask = df_missing[selected_column].isna()\n",
    "    if mask.any():\n",
    "        X_missing, _, _ = create_input_LogReg(df_missing[mask], selected_column) # create features for missing values\n",
    "        predicted = knn.predict(X_missing)\n",
    "        df_missing.loc[mask, selected_column] = le.inverse_transform(predicted) # inverse values to string labels\n",
    "\n",
    "    return df_missing\n",
    "\n",
    "\n",
    "# IMPUTING\n",
    "string_columns = ['Item', 'Payment Method', 'Location']\n",
    "numeric_columns = ['Price Per Unit', 'Quantity']\n",
    "filled_string_train = train_data_missing.copy()\n",
    "filled_numeric_train = train_data_missing.copy()\n",
    "for column in string_columns:\n",
    "    # drop selected column\n",
    "    filled_string_train.drop(column, axis=1, inplace=True)\n",
    "    temp_string_train = knn_clf(train_data_complete, train_data_missing, column)\n",
    "    filled_string_train[column] = temp_string_train[column] # add selected column back\n",
    "\n",
    "for column in numeric_columns:\n",
    "    # drop selected column\n",
    "    filled_numeric_train.drop(column, axis=1, inplace=True)\n",
    "    temp_numeric_train = knn_reg(train_data_complete, train_data_missing, column)\n",
    "    filled_numeric_train[column] = temp_numeric_train[column] # add selected column back\n",
    "\n",
    "\n",
    "# merge filled data\n",
    "filled_train = filled_string_train.drop(numeric_columns, axis=1)\n",
    "filled_train = pd.concat([filled_train, filled_numeric_train[numeric_columns]], axis=1)\n",
    "custom_knn_train_data = pd.concat([train_data_complete, filled_train])\n",
    "\n",
    "# check missing values\n",
    "missing_values_count = filled_numeric_train.isna().sum()\n",
    "print(\"\\nFILLED NUMERIC TRAIN missing values count:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "missing_values_count = filled_string_train.isna().sum()\n",
    "print(\"\\nFILLED STRING TRAIN missing values count:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "missing_values_count = custom_knn_train_data.isna().sum()\n",
    "print(\"\\nTRAIN DATA FILLED missing values count:\")\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(custom_knn_train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train and evaluate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# for column in string_columns:\n",
    "#     logistic_regression(column, custom_knn_train_data, 2000)\n",
    "\n",
    "# for column in numeric_columns:\n",
    "#     linear_regression(column, custom_knn_train_data)\n",
    "linear_regression('Quantity', custom_knn_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kết luận:**\n",
    "\n",
    "- Kết quả cho thấy tỉ lệ lỗi vẫn không đổi so với phương pháp 1, thậm chí còn thấp hơn phương pháp 2. Mặc dù mô hình KNN được huấn luyện trên một tập dữ liệu đầy đủ nhằm dự đoán các giá trị bị mất trên phần còn lại của tập train, nhưng khi dự đoán, tồn tại những mẫu bị thiếu nhiều giá trị features, và các giá trị thiếu này lại được điền bằng mode/mean nên kết quả không cải thiện nhiều so với các phương pháp trước."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **KNNImputer**\n",
    "\n",
    "Sử dụng thư viện KNNImputer để điền vào các chỗ thiếu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# import and use KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "\n",
    "# init KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "# prepare data for KNNImputer\n",
    "string_columns = ['Item', 'Payment Method', 'Location']\n",
    "imputer_train_data = train_data.copy()\n",
    "print(\"Data for KNNImputer:\", imputer_train_data.shape)\n",
    "\n",
    "# encoding string columns with one-hot encoding\n",
    "label_encoders = {}\n",
    "len_classes = {} \n",
    "for column in string_columns:\n",
    "    le = LabelEncoder()\n",
    "    imputer_train_data[column] = le.fit_transform(imputer_train_data[column])\n",
    "    label_encoders[column] = le\n",
    "    len_classes[column] = len(le.classes_)\n",
    "    print(f\"Label encoder for {column}: {le.classes_}\")\n",
    "    print(f\"\\tlen = {len(le.classes_)}\")\n",
    "\n",
    "# convert encoded NaN values to np.nan cause LabelEncoder encoded NaN values\n",
    "for column in string_columns:\n",
    "    value_to_replace = len_classes[column]-1\n",
    "    imputer_train_data[column] = imputer_train_data[column].replace(value_to_replace, np.nan)\n",
    "\n",
    "# check is converted\n",
    "print(\"\\nCheck if converted to NaN:\")\n",
    "print(imputer_train_data.head())\n",
    "\n",
    "\n",
    "\n",
    "# impute missing values\n",
    "KNNimputer_train_data = pd.DataFrame(imputer.fit_transform(imputer_train_data), columns=imputer_train_data.columns)\n",
    "\n",
    "\n",
    "# inverse label encoding for string columns\n",
    "for column in string_columns:\n",
    "    le = label_encoders[column]\n",
    "    KNNimputer_train_data[column] = le.inverse_transform(KNNimputer_train_data[column].astype(int))\n",
    "\n",
    "print(\"\\nDữ liệu sau khi điền bằng KNN:\")\n",
    "missings = KNNimputer_train_data.isna().sum()\n",
    "print(missings)\n",
    "print(KNNimputer_train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train and evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# for column in string_columns:\n",
    "#     logistic_regression(column, KNNimputer_train_data, 2000)\n",
    "\n",
    "# for column in numeric_columns:\n",
    "#     linear_regression(column, KNNimputer_train_data)\n",
    "\n",
    "linear_regression('Quantity', KNNimputer_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kết luận:**\n",
    "\n",
    "- Kết quả của mô hình vẫn không thay đổi mạnh. Cách hoạt động của KNN sẽ bỏ qua những hàng có số lượng features bị thiếu khác với cột đích, nên số lượng neighbours hợp lệ mà thuật toán tìm thấy trên tập dữ liệu này không còn nhiều, vì thế cũng không cải thiện được kết quả."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MICE**\n",
    "\n",
    "MICE coi mỗi cột có giá trị thiếu như một biến phụ thuộc và dùng các cột khác làm đặc trưng để dự đoán giá trị thiếu thông qua mô hình hồi quy (thường là hồi quy tuyến tính).\n",
    "\n",
    "Quá trình được thực hiện tuần tự (chained) qua từng cột và lặp lại nhiều lần để cải thiện kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from fancyimpute import IterativeImputer\n",
    "import pandas as pd\n",
    "\n",
    "# prepare data for MICE\n",
    "MICE_train_data = train_data.copy()\n",
    "string_columns = ['Item', 'Payment Method', 'Location']\n",
    "numeric_columns = ['Price Per Unit', 'Quantity']\n",
    "print(\"Data for MICE:\", MICE_train_data.shape)\n",
    "\n",
    "\n",
    "# encoding string columns with one-hot encoding\n",
    "label_encoders = {}\n",
    "len_classes = {} \n",
    "for column in string_columns:\n",
    "    le = LabelEncoder()\n",
    "    MICE_train_data[column] = le.fit_transform(MICE_train_data[column])\n",
    "    label_encoders[column] = le\n",
    "    len_classes[column] = len(le.classes_)\n",
    "    print(f\"Label encoder for {column}: {le.classes_}\")\n",
    "    print(f\"\\tlen = {len(le.classes_)}\")\n",
    "\n",
    "# convert encoded NaN values to np.nan cause LabelEncoder encoded NaN values\n",
    "for column in string_columns:\n",
    "    value_to_replace = len_classes[column]-1\n",
    "    MICE_train_data[column] = MICE_train_data[column].replace(value_to_replace, np.nan)\n",
    "\n",
    "# check is converted\n",
    "print(\"\\nCheck if converted to NaN:\")\n",
    "print(MICE_train_data.head())\n",
    "\n",
    "\n",
    "# init MICE\n",
    "imputer = IterativeImputer(max_iter=50, random_state=22520109)\n",
    "\n",
    "# impute\n",
    "MICE_imputed_train_data = pd.DataFrame(imputer.fit_transform(MICE_train_data), columns=MICE_train_data.columns)\n",
    "\n",
    "# inverse label encoding for string columns\n",
    "for column in string_columns:\n",
    "    le = label_encoders[column]\n",
    "    MICE_imputed_train_data[column] = le.inverse_transform(MICE_imputed_train_data[column].astype(int))\n",
    "\n",
    "print(\"\\nDữ liệu sau khi điền bằng MICE:\")\n",
    "missings = MICE_imputed_train_data.isna().sum()\n",
    "print(missings)\n",
    "print(MICE_imputed_train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train and evaluate**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# for column in string_columns:\n",
    "#     logistic_regression(column, MICE_imputed_train_data, 2000)\n",
    "\n",
    "# for column in numeric_columns:\n",
    "#     linear_regression(column, MICE_imputed_train_data)\n",
    "linear_regression('Quantity', MICE_imputed_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kết luận**\n",
    "\n",
    "- Độ lỗi của mô hình không giảm, thậm chí còn tăng thêm 1% so với KNNImputer. Điều này thể hiện mức độ ảnh hưởng của số lượng data bị mất. Dù có thực hiện các phương pháp giải quyết dữ liệu mất phức tạp hay đơn giản thì kết quả đem lại vẫn không có sự khác biệt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TỔNG KẾT**\n",
    "Trong số các phương pháp xử lý dữ liệu bị mất đã sử dụng, phương pháp *Custom KNN* và *MICE* cho ra hiệu quả tốt nhất, nhưng cũng không cách biệt quá lớn đối với các phương pháp khác. Nguyên nhân chủ yếu là do bộ dataset này bị thiếu thông tin không chỉ ở một cột, mà dữ liệu bị mất nằm rải rác ở tất cả các cột (trừ cột Transaction ID, nhưng không đem lại thông tin cho mô hình máy học). Các cột Location và Payment Method lại mất quá nhiều, khiến thông tin học được không đáng kể, hiểu quả mô hình thấp. Sau khi bỏ đi quan hệ ràng buộc giữa các cột dữ liệu số,  khả năng học của mô hình giảm đáng kể\n",
    "\n",
    "Phương pháp *Dropping rows with missing values* gây ra sự mất mát thông tin đặc biệt lớn đối với bộ dữ liệu này (mất khoảng 6000/10000 mẫu, hơn một nửa số mẫu). Nhưng kết quả chung của phương pháp này lại không quá cách biệt, bởi vì dữ liệu bị thiếu quá lớn, tập trung ở các cột không có mối quan hệ rõ ràng, nên dù được tận dụng lại bằng các phương pháp cao cấp thì vẫn không thể đem lại giá trị, thậm chí còn có thể còn gây nhiễu. \n",
    "\n",
    "Việc mất mát dữ liệu đã gây ảnh hưởng rất lớn đến chất lượng thông tin mà dataset mang lại."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
